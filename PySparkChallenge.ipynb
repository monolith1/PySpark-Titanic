{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d492a552-c338-427d-9a13-32f0f2e5e99f",
   "metadata": {},
   "source": [
    "# PySpark Challenge: Titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "331dfaa4-2c31-4b68-8873-94d60ad1f4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/31 19:02:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# create SparkContext\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "sc =SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17b3a72e-d597-4b0c-b5a1-d280aa2106b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/context.py:77: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# create sqlContext\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b106e21-f355-4409-9dc1-eb2c949dd377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv into PySpark df\n",
    "\n",
    "df = sqlContext.read.option(\"inferSchema\",True).option(\"header\",True).csv(\"titanic_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8f735d-cb43-4d39-b391-41f27aafd0d9",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "606e0883-307a-416c-be4c-601ed7e97aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see dtypes\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c32996b-6ac4-4e78-a238-932f8ce29289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+---------------------------------------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|Name                                               |Sex   |Age |SibSp|Parch|Ticket          |Fare   |Cabin|Embarked|\n",
      "+-----------+--------+------+---------------------------------------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|1          |0       |3     |Braund, Mr. Owen Harris                            |male  |22.0|1    |0    |A/5 21171       |7.25   |null |S       |\n",
      "|2          |1       |1     |Cumings, Mrs. John Bradley (Florence Briggs Thayer)|female|38.0|1    |0    |PC 17599        |71.2833|C85  |C       |\n",
      "|3          |1       |3     |Heikkinen, Miss. Laina                             |female|26.0|0    |0    |STON/O2. 3101282|7.925  |null |S       |\n",
      "|4          |1       |1     |Futrelle, Mrs. Jacques Heath (Lily May Peel)       |female|35.0|1    |0    |113803          |53.1   |C123 |S       |\n",
      "|5          |0       |3     |Allen, Mr. William Henry                           |male  |35.0|0    |0    |373450          |8.05   |null |S       |\n",
      "+-----------+--------+------+---------------------------------------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# .head()\n",
    "\n",
    "df.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c18e472f-809d-4672-a9d9-38587b073742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "|summary|      PassengerId|           Survived|            Pclass|                Name|   Sex|               Age|             SibSp|              Parch|            Ticket|             Fare|Cabin|Embarked|\n",
      "+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "|  count|              891|                891|               891|                 891|   891|               714|               891|                891|               891|              891|  204|     889|\n",
      "|   mean|            446.0| 0.3838383838383838| 2.308641975308642|                null|  null| 29.69911764705882|0.5230078563411896|0.38159371492704824|260318.54916792738| 32.2042079685746| null|    null|\n",
      "| stddev|257.3538420152301|0.48659245426485753|0.8360712409770491|                null|  null|14.526497332334035|1.1027434322934315| 0.8060572211299488|471609.26868834975|49.69342859718089| null|    null|\n",
      "|    min|                1|                  0|                 1|\"Andersson, Mr. A...|female|              0.42|                 0|                  0|            110152|              0.0|  A10|       C|\n",
      "|    max|              891|                  1|                 3|van Melkebeke, Mr...|  male|              80.0|                 8|                  6|         WE/P 5735|         512.3292|    T|       S|\n",
      "+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18e934b7-845f-487d-ad17-900fac29416a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+---+\n",
      "|sex_survived|  0|  1|\n",
      "+------------+---+---+\n",
      "|      female| 81|233|\n",
      "|        male|468|109|\n",
      "+------------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.crosstab('sex', 'survived').sort(\"sex_survived\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed2c7fb8-a7ff-4c75-89bc-2f36acf3a6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|parch|count|\n",
      "+-----+-----+\n",
      "|    0|  678|\n",
      "|    1|  118|\n",
      "|    2|   80|\n",
      "|    3|    5|\n",
      "|    4|    4|\n",
      "|    5|    5|\n",
      "|    6|    1|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('parch').count().sort('parch').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f150976f-c66e-4af1-9808-d5aa8d93cfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|sibsp|count|\n",
      "+-----+-----+\n",
      "|    0|  608|\n",
      "|    1|  209|\n",
      "|    2|   28|\n",
      "|    3|   16|\n",
      "|    4|   18|\n",
      "|    5|    5|\n",
      "|    8|    7|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('sibsp').count().sort('sibsp').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9514bb8f-0a70-4f44-b5ed-0b601d1bb0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|          0|       0|     0|   0|  0|  0|    0|    0|     0|   0|    0|       0|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find nans\n",
    "\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "df.select([count(when(isnan(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca85fb5e-f279-4a85-850d-efc4bba6a0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|          0|       0|     0|   0|  0|177|    0|    0|     0|   0|  687|       2|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find null\n",
    "\n",
    "from pyspark.sql.functions import isnull\n",
    "\n",
    "df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf9d091-ca55-4441-8a8d-a8296d933099",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48101a60-bcdb-42cb-b701-0b9f1acfbda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop cabin due to ~so many~ nulls\n",
    "\n",
    "df = df.drop('Cabin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab9a99f5-647a-41cb-8a76-f05229a2030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop name - not necessary for prediction\n",
    "\n",
    "df = df.drop('Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c93a430a-ba21-4dae-a516-327c275966e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop ticket - not necessary for prediction\n",
    "\n",
    "df = df.drop('Ticket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50f9531e-dbec-400e-9245-d7c73b2dbb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop PassengerID - not necessary for prediction\n",
    "\n",
    "df = df.drop('PassengerID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d82c0011-d7e0-4b1f-a9c6-e362c7a4b794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----+-----+-----+-------+--------+\n",
      "|Survived|Pclass|   Sex| Age|SibSp|Parch|   Fare|Embarked|\n",
      "+--------+------+------+----+-----+-----+-------+--------+\n",
      "|       0|     3|  male|22.0|    1|    0|   7.25|       S|\n",
      "|       1|     1|female|38.0|    1|    0|71.2833|       C|\n",
      "|       1|     3|female|26.0|    0|    0|  7.925|       S|\n",
      "|       1|     1|female|35.0|    1|    0|   53.1|       S|\n",
      "|       0|     3|  male|35.0|    0|    0|   8.05|       S|\n",
      "+--------+------+------+----+-----+-----+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "140c84e5-697a-43ea-b5dd-2abd4443d20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values for Age\n",
    "\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "# instantiate\n",
    "imputer = Imputer()\n",
    "\n",
    "# set input and output columns\n",
    "imputer.setInputCols(['Age'])\n",
    "imputer.setOutputCols(['age_imputed'])\n",
    "\n",
    "# apply imputer\n",
    "model = imputer.fit(df)\n",
    "df = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2718ee2f-60a6-48d6-ae83-1833b8d07149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---+---+-----+-----+----+--------+-----------+\n",
      "|Survived|Pclass|Sex|Age|SibSp|Parch|Fare|Embarked|age_imputed|\n",
      "+--------+------+---+---+-----+-----+----+--------+-----------+\n",
      "|       0|     0|  0|177|    0|    0|   0|       2|          0|\n",
      "+--------+------+---+---+-----+-----+----+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find null\n",
    "\n",
    "from pyspark.sql.functions import isnull\n",
    "\n",
    "df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7494cd2-12f2-4847-a7d4-1bcff9844a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop age (we only need age_imputed)\n",
    "\n",
    "df = df.drop('Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3e6e90a-f394-4482-83a9-a040794c3962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Sex\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"Sex\", outputCol=\"sex_encoded\")\n",
    "model = stringIndexer.fit(df)\n",
    "df = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c154173-4f8b-4de2-a831-6d601e20d10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop now rendundant sex column\n",
    "\n",
    "df = df.drop('Sex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf6d0070-74b5-4522-a296-ad6a61e95578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----+-----+-------+--------+-----------------+-----------+\n",
      "|Survived|Pclass|SibSp|Parch|   Fare|Embarked|      age_imputed|sex_encoded|\n",
      "+--------+------+-----+-----+-------+--------+-----------------+-----------+\n",
      "|       0|     3|    1|    0|   7.25|       S|             22.0|        0.0|\n",
      "|       1|     1|    1|    0|71.2833|       C|             38.0|        1.0|\n",
      "|       1|     3|    0|    0|  7.925|       S|             26.0|        1.0|\n",
      "|       1|     1|    1|    0|   53.1|       S|             35.0|        1.0|\n",
      "|       0|     3|    0|    0|   8.05|       S|             35.0|        0.0|\n",
      "|       0|     3|    0|    0| 8.4583|       Q|29.69911764705882|        0.0|\n",
      "|       0|     1|    0|    0|51.8625|       S|             54.0|        0.0|\n",
      "|       0|     3|    3|    1| 21.075|       S|              2.0|        0.0|\n",
      "|       1|     3|    0|    2|11.1333|       S|             27.0|        1.0|\n",
      "|       1|     2|    1|    0|30.0708|       C|             14.0|        1.0|\n",
      "+--------+------+-----+-----+-------+--------+-----------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1728c561-3293-48ec-8326-07a4f9a26990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop two rows with null values in embarked\n",
    "\n",
    "df = df.na.drop(subset=[\"Embarked\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84f2f044-c86d-4d1a-a49f-40254be2c853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----+-----+----+--------+-----------+-----------+\n",
      "|Survived|Pclass|SibSp|Parch|Fare|Embarked|age_imputed|sex_encoded|\n",
      "+--------+------+-----+-----+----+--------+-----------+-----------+\n",
      "|       0|     0|    0|    0|   0|       0|          0|          0|\n",
      "+--------+------+-----+-----+----+--------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a36f1d8-522d-4f28-9f98-94368ee0e11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Embarked\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder, VectorAssembler\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"Embarked\", outputCol=\"embarked_encoded\")\n",
    "model = stringIndexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "encoder = OneHotEncoder(dropLast=False, inputCol=\"embarked_encoded\", outputCol=\"embarked_vec\").fit(indexed)\n",
    "df = encoder.transform(indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a73cfe3a-cb5d-4682-9663-9ad40f8a9762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns embarked and embarked_encoded, as we only wish to keep vectorized embarked data\n",
    "\n",
    "df = df.drop('embarked')\n",
    "df = df.drop('embarked_encoded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ebc426e3-1244-4c2e-b84c-606d8bf8308f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----+-----+-------+-----------+-----------+-------------+\n",
      "|Survived|Pclass|SibSp|Parch|   Fare|age_imputed|sex_encoded| embarked_vec|\n",
      "+--------+------+-----+-----+-------+-----------+-----------+-------------+\n",
      "|       0|     3|    1|    0|   7.25|       22.0|        0.0|(3,[0],[1.0])|\n",
      "|       1|     1|    1|    0|71.2833|       38.0|        1.0|(3,[1],[1.0])|\n",
      "|       1|     3|    0|    0|  7.925|       26.0|        1.0|(3,[0],[1.0])|\n",
      "|       1|     1|    1|    0|   53.1|       35.0|        1.0|(3,[0],[1.0])|\n",
      "|       0|     3|    0|    0|   8.05|       35.0|        0.0|(3,[0],[1.0])|\n",
      "+--------+------+-----+-----+-------+-----------+-----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd8a57b9-8990-4c27-99bf-dc61ecfd8bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "assemblerInputs = ['Pclass', 'SibSp', 'Parch', 'Fare', 'age_imputed', 'sex_encoded', 'embarked_vec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ebc283b-c6a4-49f6-ac25-eed81165a2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "df = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ee2d2dbf-7bb8-42d2-8022-b30859327d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----+-----+-------+-----------+-----------+-------------+--------------------+\n",
      "|Survived|Pclass|SibSp|Parch|   Fare|age_imputed|sex_encoded| embarked_vec|            features|\n",
      "+--------+------+-----+-----+-------+-----------+-----------+-------------+--------------------+\n",
      "|       0|     3|    1|    0|   7.25|       22.0|        0.0|(3,[0],[1.0])|[3.0,1.0,0.0,7.25...|\n",
      "|       1|     1|    1|    0|71.2833|       38.0|        1.0|(3,[1],[1.0])|[1.0,1.0,0.0,71.2...|\n",
      "|       1|     3|    0|    0|  7.925|       26.0|        1.0|(3,[0],[1.0])|[3.0,0.0,0.0,7.92...|\n",
      "|       1|     1|    1|    0|   53.1|       35.0|        1.0|(3,[0],[1.0])|[1.0,1.0,0.0,53.1...|\n",
      "|       0|     3|    0|    0|   8.05|       35.0|        0.0|(3,[0],[1.0])|(9,[0,3,4,6],[3.0...|\n",
      "+--------+------+-----+-----+-------+-----------+-----------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09eb31ce-7b99-4acd-9bf4-ddaa580c70c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to dense vector\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "input_data = df.rdd.map(lambda x: (x[\"Survived\"], DenseVector(x[\"features\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b4e1d6-1533-4aab-8a5f-05b4fc88fadf",
   "metadata": {},
   "source": [
    "### Fit and Prediction - LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "998d9150-7a87-4940-986b-c774469ce2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_data, test_data = df.randomSplit([.8,.2],seed=808)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a4ff6954-7ac1-4e23-8b36-f6afa331779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `LinearRegression`\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Initialize `lr`\n",
    "lr = LogisticRegression(labelCol=\"Survived\",\n",
    "                        featuresCol=\"features\",\n",
    "                        maxIter=10,\n",
    "                        regParam=0.3)\n",
    "\n",
    "# Fit the data to the model\n",
    "linearModel = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac0d3805-0bec-4d8e-b0c0-ad32ff7018e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-0.28245622608820803,-0.06590005996362967,0.033547311904343645,0.002397111864947558,-0.008231932747059299,0.9936720840407006,-0.1861644324436586,0.22654112571168128,0.03577423765134244]\n",
      "Intercept: 0.07838496194652372\n"
     ]
    }
   ],
   "source": [
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(linearModel.coefficients))\n",
    "print(\"Intercept: \" + str(linearModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "da28a78c-02af-4436-9440-8f0dcfd69ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data using the transform() method.\n",
    "predictions = linearModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "de02093d-d872-4338-aae2-d240685f50de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+\n",
      "|survived|prediction|         probability|\n",
      "+--------+----------+--------------------+\n",
      "|       0|       0.0|[0.70156355193123...|\n",
      "|       0|       0.0|[0.70300903057448...|\n",
      "|       0|       0.0|[0.59478640032728...|\n",
      "|       0|       0.0|[0.54430737577575...|\n",
      "|       0|       0.0|[0.67049834347800...|\n",
      "|       0|       0.0|[0.58187795902624...|\n",
      "|       0|       0.0|[0.61898819410207...|\n",
      "|       0|       0.0|[0.54981276166594...|\n",
      "|       0|       0.0|[0.68853944905941...|\n",
      "|       0|       0.0|[0.68853944905941...|\n",
      "|       0|       0.0|[0.74316737238060...|\n",
      "|       0|       0.0|[0.70000628457327...|\n",
      "|       0|       0.0|[0.72363703581669...|\n",
      "|       0|       0.0|[0.60186795409532...|\n",
      "|       0|       0.0|[0.70725003832886...|\n",
      "|       0|       0.0|[0.72235086827777...|\n",
      "|       0|       0.0|[0.73855509207932...|\n",
      "|       0|       0.0|[0.61273559127477...|\n",
      "|       0|       0.0|[0.75128422672661...|\n",
      "|       0|       0.0|[0.79551789111193...|\n",
      "+--------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected = predictions.select(\"survived\", \"prediction\", \"probability\")\n",
    "selected.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c18b7484-2a2c-4440-9b05-8039b6056575",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = predictions.select(\"survived\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "03cc6f1a-bb06-4b76-b2b8-a4b9928335ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n",
      "|survived|count(survived)|\n",
      "+--------+---------------+\n",
      "|       1|             61|\n",
      "|       0|            107|\n",
      "+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm.groupby('survived').agg({'survived': 'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "49d8fa9c-896c-41b6-9364-c3de84158fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7857142857142857"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy\n",
    "cm.filter(cm.survived == cm.prediction).count() / cm.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d97bdc27-9ade-43f3-b98e-200c5b09b656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 78.571%\n"
     ]
    }
   ],
   "source": [
    "def accuracy_m(model): \n",
    "    predictions = model.transform(test_data)\n",
    "    cm = predictions.select(\"survived\", \"prediction\")\n",
    "    acc = cm.filter(cm.survived == cm.prediction).count() / cm.count()\n",
    "    print(\"Model accuracy: %.3f%%\" % (acc * 100)) \n",
    "accuracy_m(model = linearModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaf3f83-9e5f-4d35-acd2-1936cb9fed5f",
   "metadata": {},
   "source": [
    "### Fit and Prediction - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b8e0fc91-4921-4b57-8ba8-5ab88bf80d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'Survived')\n",
    "\n",
    "# Fit the data to the model\n",
    "RFModel = rf.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3a110425-6796-406b-ac75-7780e641b651",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_rf = RFModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "957b8770-237e-42fe-82ce-c1dd241224cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+\n",
      "|survived|prediction|         probability|\n",
      "+--------+----------+--------------------+\n",
      "|       0|       0.0|[0.80884837119048...|\n",
      "|       0|       0.0|[0.72414880521785...|\n",
      "|       0|       0.0|[0.73590509766430...|\n",
      "|       0|       0.0|[0.69470968777086...|\n",
      "|       0|       0.0|[0.71565489530881...|\n",
      "|       0|       0.0|[0.70866309572426...|\n",
      "|       0|       0.0|[0.65000012396856...|\n",
      "|       0|       0.0|[0.57941082209155...|\n",
      "|       0|       0.0|[0.81984058468933...|\n",
      "|       0|       0.0|[0.81984058468933...|\n",
      "|       0|       0.0|[0.85378678394870...|\n",
      "|       0|       0.0|[0.85187910910478...|\n",
      "|       0|       0.0|[0.85378678394870...|\n",
      "|       0|       0.0|[0.78596085493210...|\n",
      "|       0|       0.0|[0.82739129625827...|\n",
      "|       0|       0.0|[0.82739129625827...|\n",
      "|       0|       0.0|[0.82739129625827...|\n",
      "|       0|       0.0|[0.74924035293360...|\n",
      "|       0|       0.0|[0.79453288698681...|\n",
      "|       0|       0.0|[0.90234438272336...|\n",
      "+--------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected = predictions_rf.select(\"survived\", \"prediction\", \"probability\")\n",
    "selected.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0389414e-9b1b-4ed2-a4e5-6aba55f5fdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_rf = predictions_rf.select(\"survived\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3a7d08a3-7997-401a-8620-6e8f5175b241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n",
      "|survived|count(survived)|\n",
      "+--------+---------------+\n",
      "|       1|             61|\n",
      "|       0|            107|\n",
      "+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm_rf.groupby('survived').agg({'survived': 'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d5309665-a723-4cca-880c-ea7b8f4054df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 79.167%\n"
     ]
    }
   ],
   "source": [
    "accuracy_m(model = RFModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb98495-6e42-4e40-8c59-b5b1c39ed948",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
